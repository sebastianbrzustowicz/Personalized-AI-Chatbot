# Personalized-AI-Chatbot
An end-to-end AI assistant with Retrieval-Augmented Generation, designed for seamless integration with other projects.

<figure>
  <p style="display: flex; justify-content: space-between;">
    <img src="images/react_1.png" style="width: 32%;" />
    <img src="images/react_2.png" style="width: 32%;" />
    <img src="images/react_3.png" style="width: 32%;" />
  </p>
  <figcaption style="text-align: center;">
    React implementation - Night Mode
  </figcaption>
</figure>

<figure>
  <p style="display: flex; justify-content: space-between;">
    <img src="images/chatbot_1.png" style="width: 32%;" />
    <img src="images/chatbot_2.png" style="width: 32%;" />
    <img src="images/chatbot_3.png" style="width: 32%;" />
  </p>
  <figcaption style="text-align: center;">
    React implementation - Light Mode
  </figcaption>
</figure>

<figure>
  <p style="display: flex; justify-content: space-between;">
    <img src="images/vanilla_1.png" style="width: 32%;" />
    <img src="images/vanilla_2.png" style="width: 32%;" />
    <img src="images/vanilla_3.png" style="width: 32%;" />
  </p>
  <figcaption style="text-align: center;">
    Vanilla JavaScript
  </figcaption>
</figure>

## Architecture
The architecture is quite simple and modular, making it easy to reuse individual components 
or integrate the chatbot into other projects with minimal setup.
```
User â†’ React Frontend â†’ FastAPI (backend) â†’ RAG Pipeline 
  â†’ Embeddings via SentenceTransformers â†’ Vector Store (ChromaDB) â†’ LLM Response
```

## Project structure
``` yaml
Personalized-AI-Chatbot/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                     # FastAPI entrypoint, starts the server
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py               # Global config (CORS, settings, env vars)
â”‚   â”‚   â””â”€â”€ logger.py               # Centralized logging setup (standard/logger)
â”‚   â”œâ”€â”€ data_preprocessing/
â”‚   â”‚   â”œâ”€â”€ file_loader.py          # Load documents (txt, pdf) into memory
â”‚   â”‚   â”œâ”€â”€ text_cleaning.py        # Clean text (remove unwanted chars, normalize)
â”‚   â”‚   â””â”€â”€ text_splitter.py        # Split text into chunks for embeddings
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ embedder.py             # Embedding model init & helper functions
â”‚   â”‚   â”œâ”€â”€ llm_model.py            # Local LLM loading & text generation
â”‚   â”‚   â”œâ”€â”€ pipeline.py             # RAGPipeline with retrieve/generate/answer
â”‚   â”‚   â””â”€â”€ vectorstore.py          # Vectorstore management (add/retrieve embeddings)
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ chat.py                 # /ask endpoint for AI chatbot
â”‚   â”‚   â””â”€â”€ health.py               # /health endpoint for health checks
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ rag_service.py          # Singleton RAGPipeline init for DI
â”‚   â”œâ”€â”€ Dockerfile                  # Dockerization instructions
â”‚   â”œâ”€â”€ requirements.txt            # Backend Python dependencies
â”‚   â””â”€â”€ server.py                   # Optional server startup logic (uvicorn)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ docs/
â”‚       â””â”€â”€ ...                     # Source documents for RAG (txt, pdf, doc, html)
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ public/                     # Static assets (favicon, index.html)
â”‚   â”œâ”€â”€ src/components/AIChatWindow/
â”‚   â”‚   â”œâ”€â”€ AIChatWindow.tsx        # Main React component for chat widget
â”‚   â”‚   â””â”€â”€ AIChatLoader.tsx        # Custom loader/spinner for AI typing
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â””â”€â”€ useAIChat.ts            # Custom hook for sending messages to backend
â”‚   â”œâ”€â”€ styles/
â”‚   â”‚   â”‚   AIChatLoader.css        # Loader-specific styles
â”‚   â”‚   â””â”€â”€ AIChatWindow.css        # Chat window styles
â”‚   â”œâ”€â”€ Dockerfile                  # Dockerization instructions
â”‚   â””â”€â”€ main.tsx                    # Frontend entrypoint
â”œâ”€â”€ llm-models/
â”‚   â””â”€â”€ ...                         # Local LLM model files (gguf, pt, bin)
â”œâ”€â”€ screenshots/                    # Screenshots for README/docs
â”œâ”€â”€ .gitignore                      # Git ignore rules
â”œâ”€â”€ docker-compose.yml              # Docker compose for backend/frontend
â”œâ”€â”€ LICENSE                         # Project license
â””â”€â”€ README.md                       # Project documentation
```

## Key features
- ğŸ§  Retrieval-Augmented Generation (RAG) â€” combines document search with LLM reasoning
- ğŸ“„ Multi-format data ingestion â€” supports .pdf, .txt, .docx, .html
- âš¡ Local vector store using ChromaDB for efficient retrieval
- ğŸ§© Plug-and-play model loading (local GGUF or HuggingFace models)
- ğŸ–¥ï¸ Interactive React frontend with live chat interface
- ğŸ³ Fully containerized (Docker + Compose) for consistent deployment
- ğŸ”Œ Seamless integration â€” drop-in React component (AIChatWindow) can be easily copied
into other web project

## Installation / Local Setup
Instructions step-by-step:
```bash
# Clone repository
git clone https://github.com/sebastianbrzustowicz/Personalized-AI-Chatbot.git
cd Personalized-AI-Chatbot
```
1ï¸âƒ£ Add your LLM model  
Place your model file in /llm-models/ and set its path in
backend/rag/pipeline.py, e.g.:
```python
self.llm = LocalLLM(model_path or "./llm_models/llama-2-7b-chat.Q4_K_M/llama-2-7b-chat.Q4_K_M.gguf")
```
2ï¸âƒ£ Add your documents  
Put your source files (PDF, TXT, DOCX, etc.) into /data/docs/.

3ï¸âƒ£ Run the app
```bash
# Run containers
docker compose up --build
```
Then open http://localhost:8000
 in your browser.

## License

Personalized-AI-Chatbot is released under the MIT license.

## Author

Sebastian Brzustowicz &lt;Se.Brzustowicz@gmail.com&gt;

